{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dbGaP - phs002249\n",
    "## KULMAP - Human Kidney, Urinary Tract, and Lung Mapping Center\n",
    "[Study Link](https://www.ncbi.nlm.nih.gov/projects/gapprev/gap/cgi-bin/study.cgi?study_id=phs002249.v2.p1)\n",
    "\n",
    "The major goal of the Human Kidney, Urinary Tract, and Lung Mapping Center (KULMAP) is to generate multi-omic and spatially resolved molecular anatomical maps of the human bladder, ureter and kidneys (BUKMAP) and the lung airways and parenchyma (LAPMAP) at a single cell resolution. This entails sequencing of the transcriptomes and epigenomes of dissociated single cells in a massively parallel manner. These profiles will then inform on highly multiplexed RNA in situ imaging for spatial mapping of hundreds of molecular targets in the tissue sections, at a subcellular resolution. These spatial molecular maps will serve as scaffolds for computational registration of cell types and the associated transcriptome/chromatin maps to the tissue space. The combination of sequencing single dissociated cells and multiplexed in situ mapping will allow the construction of detailed spatial maps for these large organs.\n",
    "\n",
    "### PI\n",
    "* Kun Zhang. University of California, San Diego, La Jolla, CA, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shutil import copytree\n",
    "from shutil import rmtree\n",
    "import pathlib\n",
    "import json\n",
    "import yaml\n",
    "import hubmapbags\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from warnings import warn as warning\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "#for publishing only use the prod instance\n",
    "instance = 'prod'\n",
    "\n",
    "# get token from ingest.hubmapconsortium.org\n",
    "token = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "output_directory = 'data'\n",
    "if not Path(output_directory).exists():\n",
    "    Path(output_directory).mkdir()\n",
    "\n",
    "report_output_directory = 'uuid-protected-data-report'\n",
    "if not Path(report_output_directory).exists():\n",
    "    Path(report_output_directory).mkdir()\n",
    "\n",
    "report_output_filename = report_output_directory + '/' + str(now.strftime('%Y%m%d')) + '.tsv'\n",
    "    \n",
    "def is_primary( hubmap_id, instance='prod', token=None ):\n",
    "    metadata = hubmapbags.apis.get_ancestors_info( hubmap_id, instance=instance, token=token )\n",
    "    if 'entity_type' in metadata[0].keys() and  metadata[0]['entity_type'] == 'Sample':\n",
    "        return True\n",
    "    else:\n",
    "        if 'error' in metadata[0]:\n",
    "            warning(metadata[0]['error'])\n",
    "        return False\n",
    "    \n",
    "def has_metadata( metadata ):\n",
    "    if 'ingest_metadata' in metadata.keys() and 'metadata' in metadata['ingest_metadata'].keys():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(report_output_filename).exists():\n",
    "    # get assay types\n",
    "    assay_names = hubmapbags.apis.get_assay_types( token=token )\n",
    "\n",
    "    report = pd.DataFrame()\n",
    "    for assay_name in assay_names:\n",
    "        print(assay_name)\n",
    "        datasets = pd.DataFrame(hubmapbags.get_hubmap_ids( assay_name=assay_name, token=token ))\n",
    "\n",
    "        if datasets.empty:\n",
    "            continue\n",
    "\n",
    "        #clean up\n",
    "        datasets = datasets[(datasets['data_type'] != 'image_pyramid')]\n",
    "        datasets = datasets[(datasets['status'] == 'Published')]\n",
    "        \n",
    "        for index, datum in tqdm(datasets.iterrows()):\n",
    "            datasets.loc[index, 'directory'] = hubmapbags.apis.get_directory( datum['hubmap_id'], instance='prod', token=token )\n",
    "            metadata = hubmapbags.apis.get_dataset_info( datum['hubmap_id'], instance='prod', token=token )\n",
    "            \n",
    "            if has_metadata( metadata ):\n",
    "                datasets.loc[index,'has_metadata'] = True\n",
    "                if 'ingest_metadata' in metadata.keys():\n",
    "                    datasets.loc[index,'assay_type'] = metadata['ingest_metadata']['metadata']['assay_type']\n",
    "                    datasets.loc[index,'assay_category'] = metadata['ingest_metadata']['metadata']['assay_category']\n",
    "                else:\n",
    "                    datasets.loc[index,'assay_type'] = None\n",
    "                    datasets.loc[index, 'assay_category'] = None\n",
    "            else:\n",
    "                datasets.loc[index,'has_metadata'] = False\n",
    "            \n",
    "        report = pd.concat([report,datasets])\n",
    "    \n",
    "    report = report[['group_name','uuid','hubmap_id','status','is_protected','data_type','assay_type','assay_category','directory']]\n",
    "    report.to_csv(report_output_filename, sep='\\t', index=False)\n",
    "    report.to_pickle(reportf_output_filename.replace('tsv','pkl'))\n",
    "else:\n",
    "    print('File found on disk. Loading ' + report_output_filename + '.')\n",
    "    report = pd.read_csv(report_output_filename, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = report[(report['group_name'] == 'University of California San Diego TMC') | ((report['group_name'] == 'Broad Institute RTI') & (report['data_type'] == 'Slide-seq'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not Path('dataframe.tsv').exists():\n",
    "    columns = ['phs_accession','sample_ID','library_ID', \\\n",
    "               'title','group_name','data_type','library_strategy', \\\n",
    "               'library_source','library_selection', \\\n",
    "               'library_layout','platform','instrument_model', \\\n",
    "               'design_description','reference_genome_assembly', \\\n",
    "               'alignment_software']\n",
    "\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    phs_accession = 'phs002249'\n",
    "\n",
    "    rows = []\n",
    "    lib_counter = 1\n",
    "    for index, datum in tqdm(report.iterrows()):\n",
    "        metadata = hubmapbags.apis.get_dataset_info(datum['hubmap_id'], instance='prod', token=token)\n",
    "        pmetadata = hubmapbags.apis.get_provenance_info(datum['hubmap_id'], instance='prod', token=token)\n",
    "\n",
    "        organ = pmetadata['organ_type'][0]\n",
    "\n",
    "        try:\n",
    "            library_ID = metadata['ingest_metadata']['metadata']['library_id']\n",
    "            library_ID = library_ID + '-' + datum['hubmap_id']\n",
    "        except:\n",
    "            library_ID = 'lib-' + datum['hubmap_id']\n",
    "\n",
    "        try:\n",
    "            instrument_model = metadata['ingest_metadata']['metadata']['acquisition_instrument_model']\n",
    "        except:\n",
    "            instrument_model = ''\n",
    "\n",
    "        try:\n",
    "            platform = metadata['ingest_metadata']['metadata']['acquisition_instrument_vendor']\n",
    "        except:\n",
    "            platform = ''\n",
    "\n",
    "        try:\n",
    "            if metadata['ingest_metadata']['metadata']['library_layout'] == 'paired-end':\n",
    "                library_layout = 'paired'\n",
    "            else:\n",
    "                library_layout = 'single'\n",
    "        except:\n",
    "            library_layout = ''\n",
    "\n",
    "        library_selection = 'other'\n",
    "\n",
    "        if datum['data_type'] == 'Slide-seq':\n",
    "            library_strategy = 'RNA-Seq'\n",
    "            title='Slide-seq of human ' + organ \n",
    "            source = 'TRANSCRIPTOMIC'\n",
    "        elif datum['data_tfype'] == 'snRNAseq':\n",
    "            library_strategy = 'RNA-Seq'\n",
    "            title='snRNAseq-10xGenomics-v3 of human ' + organ \n",
    "            source = 'TRANSCRIPTOMIC'\n",
    "        else:\n",
    "            try:\n",
    "                if metadata['ingest_metadata']['metadata']['analyte_class'] == 'DNA':\n",
    "                    library_strategy = 'ATAC-seq'\n",
    "                    title='SNARE2-ATAC-seq of human ' + organ\n",
    "                    library_source = 'GENOMIC'\n",
    "                else:\n",
    "                    library_strategy = 'RNA-Seq'\n",
    "                    title='SNARE2-RNA-Seq of human ' + organ\n",
    "                    library_source = 'TRANSCRIPTOMIC'\n",
    "            except:\n",
    "                library_strategy = ''\n",
    "                title=''\n",
    "                library_source = ''\n",
    "\n",
    "        if datum['data_type'] == 'Slide-seq':\n",
    "            library_construction_protocols_io_doi = metadata['ingest_metadata']['metadata']['library_construction_protocols_io_doi']\n",
    "            sequencing_reagent_kit = metadata['ingest_metadata']['metadata']['sequencing_reagent_kit']\n",
    "            design_description = f'The protocol and materials for the Slide-seq library construction process can be found in the following protocols.io protocol: dx.doi.org/{library_construction_protocols_io_doi}. The library was sequenced on the {platform} {instrument_model} system using the {sequencing_reagent_kit} kit.'\n",
    "        else:\n",
    "            try:\n",
    "                library_construction_protocols_io_doi = metadata['ingest_metadata']['metadata']['library_construction_protocols_io_doi']\n",
    "                sequencing_reagent_kit = metadata['ingest_metadata']['metadata']['sequencing_reagent_kit']\n",
    "                design_description = f'SNARE-seq2 was performed as outlined (Nature Protocols, DOI:10.1038/s41596-021-00507-3) and according to the following protocols.io protocol: dx.doi.org/{library_construction_protocols_io_doi}. The library was sequenced on the {platform} {instrument_model} system using the {sequencing_reagent_kit} kit.'\n",
    "            except:\n",
    "                design_description = ''\n",
    "\n",
    "        row = {'phs_accession':phs_accession, \\\n",
    "               'sample_ID':datum['hubmap_id'], \\\n",
    "               'library_ID':library_ID, \\\n",
    "               'title':title, \\\n",
    "               'group_name':datum['group_name'], \\\n",
    "               'data_type':datum['data_type'], \\\n",
    "               'library_strategy':library_strategy , \\\n",
    "               'library_source':library_source , \\\n",
    "               'library_selection':library_selection, \\\n",
    "               'library_layout':library_layout, \\\n",
    "               'platform':platform, \\\n",
    "               'instrument_model':instrument_model, \\\n",
    "               'design_description':design_description, \\\n",
    "               'reference_genome_assembly':None, \\\n",
    "               'alignment_software':None }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    for row in rows:\n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "    df.to_csv( 'dataframe.tsv', sep='\\t', index=False )\n",
    "else:\n",
    "    df = pd.read_csv( 'dataframe.tsv', sep='\\t' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('files.pkl', 'rb')\n",
    "files = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filetype0\"]=None\n",
    "df[\"filename0\"]=None\n",
    "df[\"checksum0\"]=None\n",
    "df[\"filetype1\"]=None\n",
    "df[\"filename1\"]=None\n",
    "df[\"checksum1\"]=None\n",
    "df[\"filetype2\"]=None\n",
    "df[\"filename2\"]=None\n",
    "df[\"checksum2\"]=None\n",
    "df[\"filetype3\"]=None\n",
    "df[\"filename3\"]=None\n",
    "df[\"checksum3\"]=None\n",
    "df[\"filetype4\"]=None\n",
    "df[\"filename4\"]=None\n",
    "df[\"checksum4\"]=None\n",
    "df[\"filetype5\"]=None\n",
    "df[\"filename5\"]=None\n",
    "df[\"checksum5\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = []\n",
    "for f in files:\n",
    "    numbers.append(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, datum in df.iterrows():\n",
    "    if len(files[index]) == 2:\n",
    "        keys = list(files[index].keys())\n",
    "        df.loc[index,'checksum0'] = files[index][keys[0]]\n",
    "        df.loc[index,'filename0'] = keys[0] + '.gz'\n",
    "        df.loc[index,'checksum1'] = files[index][keys[1]]\n",
    "        df.loc[index,'filename1'] = keys[1] + '.gz'\n",
    "        df.loc[index,'filetype0'] = df.loc[index,'filetype1'] = 'fastq'\n",
    "    elif len(files[index]) == 3:\n",
    "        keys = list(files[index].keys())\n",
    "        df.loc[index,'checksum0'] = files[index][keys[0]]\n",
    "        df.loc[index,'filename0'] = keys[0] + '.gz'\n",
    "        df.loc[index,'checksum1'] = files[index][keys[1]]\n",
    "        df.loc[index,'filename1'] = keys[1] + '.gz'\n",
    "        df.loc[index,'checksum2'] = files[index][keys[2]]\n",
    "        df.loc[index,'filename2'] = keys[2] + '.gz'\n",
    "        df.loc[index,'filetype0'] = df.loc[index,'filetype1'] = df.loc[index,'filetype2'] = 'fastq'\n",
    "    elif len(files[index]) == 4:\n",
    "        keys = list(files[index].keys())\n",
    "        df.loc[index,'checksum0'] = files[index][keys[0]]\n",
    "        df.loc[index,'filename0'] = keys[0] + '.gz'\n",
    "        df.loc[index,'checksum1'] = files[index][keys[1]]\n",
    "        df.loc[index,'filename1'] = keys[1] + '.gz'\n",
    "        df.loc[index,'checksum2'] = files[index][keys[2]]\n",
    "        df.loc[index,'filename2'] = keys[2] + '.gz'\n",
    "        df.loc[index,'checksum3'] = files[index][keys[3]]\n",
    "        df.loc[index,'filename3'] = keys[3] + '.gz'\n",
    "        df.loc[index,'filetype0'] = df.loc[index,'filetype1'] = df.loc[index,'filetype2'] = df.loc[index,'filetype3'] = 'fastq'\n",
    "    else:\n",
    "        keys = list(files[index].keys())\n",
    "        df.loc[index,'checksum0'] = files[index][keys[0]]\n",
    "        df.loc[index,'filename0'] = keys[0] + '.gz'\n",
    "        df.loc[index,'checksum1'] = files[index][keys[1]]\n",
    "        df.loc[index,'filename1'] = keys[1] + '.gz'\n",
    "        df.loc[index,'checksum2'] = files[index][keys[2]]\n",
    "        df.loc[index,'filename2'] = keys[2] + '.gz'\n",
    "        df.loc[index,'checksum3'] = files[index][keys[3]]\n",
    "        df.loc[index,'filename3'] = keys[3] + '.gz'\n",
    "        df.loc[index,'checksum4'] = files[index][keys[4]]\n",
    "        df.loc[index,'filename4'] = keys[4] + '.gz'\n",
    "        df.loc[index,'checksum5'] = files[index][keys[5]]\n",
    "        df.loc[index,'filename5'] = keys[5] + '.gz'\n",
    "        df.loc[index,'filetype0'] = df.loc[index,'filetype1'] = df.loc[index,'filetype2'] = df.loc[index,'filetype3'] = df.loc[index,'filetype4'] =  df.loc[index,'filetype5'] = 'fastq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('phs002249.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
